{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance  = pd.read_csv('train/performance_train.csv', index_col= False)\n",
    "facturation  = pd.read_csv('train/facturation_train.csv', index_col= False)\n",
    "paiements    = pd.read_csv('train/paiements_train.csv', index_col= False)\n",
    "transactions = pd.read_csv('train/transactions_train.csv', index_col= False)\n",
    "#load test dataset\n",
    "performance_test  = pd.read_csv('test/performance_test.csv', index_col= False)\n",
    "facturation_test  = pd.read_csv('test/facturation_test.csv', index_col= False)\n",
    "paiements_test    = pd.read_csv('test/paiements_test.csv', index_col= False)\n",
    "transactions_test = pd.read_csv('test/transactions_test.csv', index_col= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_by_ID(dataframe):\n",
    "    output = {}\n",
    "    DECISION_keys = dataframe[\"DECISION_XCD\"].value_counts().keys()\n",
    "    TRANSACTION_C_keys = dataframe[\"TRANSACTION_CATEGORY_XCD\"].value_counts().keys()\n",
    "    TRANSACTION_T_keys = dataframe[\"TRANSACTION_TYPE_XCD\"].value_counts().keys()\n",
    "    cmt = dataframe[\"cred_minus_transaction_net_positive\"].value_counts().keys()\n",
    "    SICGROUP_keys = dataframe[\"SICGROUP\"].value_counts().keys()\n",
    "    for i in dataframe[\"ID_CPTE\"].value_counts().keys():        \n",
    "        subframe = dataframe.loc[dataframe[\"ID_CPTE\"] == i]\n",
    "        #query for DECISION_XCD\n",
    "        DECISION_dict = {}\n",
    "        for j in DECISION_keys:\n",
    "            s = \"DECISION_XCD_\" + j\n",
    "            try:\n",
    "                DECISION_dict[s] = subframe[\"DECISION_XCD\"].value_counts(normalize=True)[j]\n",
    "            except:\n",
    "                DECISION_dict[s] = 0\n",
    "        #query for transaction_c\n",
    "        TRANSACTION_C_dict = {}\n",
    "        for j in TRANSACTION_C_keys:\n",
    "            s = \"TRANSACTION_C_\" + j\n",
    "            try:\n",
    "                TRANSACTION_C_dict[s] = subframe[\"TRANSACTION_CATEGORY_XCD\"].value_counts(normalize=True)[j]\n",
    "            except:\n",
    "                TRANSACTION_C_dict[s] = 0\n",
    "        TRANSACTION_T_dict = {}\n",
    "        #query for transaction_t    \n",
    "        for j in TRANSACTION_T_keys:\n",
    "            s = \"TRANSACTION_T_\" + j\n",
    "            try:\n",
    "                TRANSACTION_T_dict[s] = subframe[\"TRANSACTION_TYPE_XCD\"].value_counts(normalize=True)[j]\n",
    "            except:\n",
    "                TRANSACTION_T_dict[s] = 0\n",
    "        #query for SICGROUP\n",
    "        SICGROUP_dict = {}\n",
    "\n",
    "        for j in SICGROUP_keys:\n",
    "            s = \"SCIGROUP_\" + j\n",
    "            try:\n",
    "                SICGROUP_dict[s] = subframe[\"SICGROUP\"].value_counts(normalize=True)[j]\n",
    "            except:\n",
    "                SICGROUP_dict[s] = 0\n",
    "        CMT_dict = {}\n",
    "        for j in cmt:\n",
    "            s = \"cred_minus_transaction_net_positive\" + str(j)\n",
    "            try:\n",
    "                CMT_dict[s] = subframe[\"cred_minus_transaction_net_positive\"].value_counts(normalize=True)[j]\n",
    "            except:\n",
    "                CMT_dict[s] = 0\n",
    "                \n",
    "        output[i] = [DECISION_dict, TRANSACTION_C_dict, TRANSACTION_T_dict, SICGROUP_dict, CMT_dict]\n",
    "    return output\n",
    "def summarize_by_ID_2(dataframe):\n",
    "    output = {}\n",
    "    PAYMENT_REVERSAL_XFLG_key =  dataframe[\"PAYMENT_REVERSAL_XFLG\"].value_counts().keys()\n",
    "    for i in dataframe[\"ID_CPTE\"].value_counts().keys():        \n",
    "        subframe = dataframe.loc[dataframe[\"ID_CPTE\"] == i]\n",
    "        TRANSACTION_SUM_dict = {}\n",
    "        TRANSACTION_SUM_dict[\"TRANSACTION_AMT_sum\"] = subframe[\"TRANSACTION_AMT\"].sum()\n",
    "\n",
    "        PAYMENT_REVERSAL_XFLG_dict = {}\n",
    "        for j in PAYMENT_REVERSAL_XFLG_key:\n",
    "            s = \"PAYMENT_REVERSAL_XFLG_key_\" + str(j)\n",
    "            try:\n",
    "                PAYMENT_REVERSAL_XFLG_dict[s] = subframe[\"PAYMENT_REVERSAL_XFLG\"].value_counts(normalize=True)[j]\n",
    "            except:\n",
    "                PAYMENT_REVERSAL_XFLG_dict[s] = 0   \n",
    "        output[i] = [TRANSACTION_SUM_dict,PAYMENT_REVERSAL_XFLG_dict]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID_of_defaults = performance.loc[performance[\"Default\"] == 1][\"ID_CPTE\"]\n",
    "#Uniform across all years so drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_engineering(performance,paiements,transactions,test):\n",
    "    performance[\"PERIODID_MY\"]= pd.to_datetime(performance[\"PERIODID_MY\"]).dt.year\n",
    "    #Get rid of BS features\n",
    "    transaction_dropped = transactions.drop([\"MERCHANT_CITY_NAME\",\"MERCHANT_CATEGORY_XCD\",\"MERCHANT_COUNTRY_XCD\", \"TRANSACTION_DTTM\"],1)\n",
    "    \n",
    "    ## add credit limit minus transaction amount and drop credit limit, transaction amount\n",
    "    cred_minus_transaction = transaction_dropped[\"PRIOR_CREDIT_LIMIT_AMT\"].sub(transaction_dropped[\"TRANSACTION_AMT\"])\n",
    "    transaction_dropped = transaction_dropped.drop([\"PRIOR_CREDIT_LIMIT_AMT\", \"TRANSACTION_AMT\"],1)\n",
    "    transaction_dropped['cred_minus_transaction'] = cred_minus_transaction\n",
    "    \n",
    "    # drop cred_minus_transaction and query whether it is positive\n",
    "    transaction_dropped[\"cred_minus_transaction_net_positive\"] = transaction_dropped[\"cred_minus_transaction\"].ge(0)\n",
    "    transaction_dropped = transaction_dropped.drop([\"cred_minus_transaction\"],1)\n",
    "    \n",
    "    ##Create cleaned dataframe for transaction \n",
    "    output = summarize_by_ID(transaction_dropped)\n",
    "    convert = {}\n",
    "    s = pd.Series()\n",
    "    for i in output.keys():\n",
    "        for k in output[i]:\n",
    "            s= {**s,**k}\n",
    "        convert[i] = pd.Series(s)\n",
    "    final = pd.DataFrame.from_dict(convert, orient='index')\n",
    "    \n",
    "    #create cleaned dataframe for payments\n",
    "\n",
    "    paiements_drop = paiements.drop([\"TRANSACTION_DTTM\"],1)\n",
    "    \n",
    "    output2 = summarize_by_ID_2(paiements_drop)\n",
    "    convert2 = {}\n",
    "    s2 = pd.Series()\n",
    "    for i in output2.keys():\n",
    "        for k2 in output2[i]:\n",
    "            s2= {**s2,**k2}\n",
    "        convert2[i] = pd.Series(s2)    \n",
    "    final2 = pd.DataFrame.from_dict(convert2, orient='index')\n",
    "    \n",
    "    #create cleaned dataframe for performance\n",
    "    temp = performance.set_index(\"ID_CPTE\")\n",
    "    del temp.index.name\n",
    "    \n",
    "    combined = final2.combine_first(final.combine_first(temp))\n",
    "    if (not test):\n",
    "        combined_drop_features = combined[[\"cred_minus_transaction_net_positiveTrue\",\"Default\", \"PAYMENT_REVERSAL_XFLG_key_Q\"]]\n",
    "    else:\n",
    "        combined_drop_features = combined[[\"cred_minus_transaction_net_positiveTrue\", \"PAYMENT_REVERSAL_XFLG_key_Q\"]]\n",
    "        \n",
    "    return combined_drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imputing(dataset_train_x, imputee):\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp = imp.fit(dataset_train_x)\n",
    "    return imp.transform(imputee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_boosting_classifier(train,test):\n",
    "    space  = [Integer(2, 200, name='max_depth'),\n",
    "              Real(10**-5, 10**0, \"log-uniform\", name='learning_rate'),\n",
    "              Integer(1, train_x.shape[1], name='max_features'),\n",
    "              Integer(2, 100, name='min_samples_split'),\n",
    "              Integer(1, 100, name='min_samples_leaf')]    \n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        reg.set_params(**params)\n",
    "\n",
    "        return -np.mean(cross_val_score(reg, train,test , cv=5, n_jobs=-1,\n",
    "                                        scoring=\"neg_mean_absolute_error\"))\n",
    "    reg = GradientBoostingClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "    res_gp = gp_minimize(objective, space, n_calls=50, random_state=0)\n",
    "    return GradientBoostingClassifier(n_estimators=50, random_state=0, max_depth = res_gp.x[0], \n",
    "                                      learning_rate = res_gp.x[1], max_features = res_gp.x[2], min_samples_split = res_gp.x[3]\n",
    "                                     ,min_samples_leaf= res_gp.x[4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(classifier, X):\n",
    "    return classifier.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission_creator(ID, default):\n",
    "    return pd.concat([pd.DataFrame(ID),pd.DataFrame(default)],axis =1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_write(dataframe):\n",
    "    dataframe.to_csv(\"submission.csv\", index=False, header =['ID_CPTE', 'Default'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = feature_engineering(performance,paiements,transactions, False)\n",
    "\n",
    "#whole dataset split x,y\n",
    "dataset_train_x, dataset_train_y =  dataset_train.drop([\"Default\"],1), dataset_train[\"Default\"]\n",
    "\n",
    "##### Training dataset created #####\n",
    "#dataset split training and validation\n",
    "\n",
    "train, valid = train_test_split(dataset_train, test_size=0.2)\n",
    "train_y = train[\"Default\"]\n",
    "train_x = train.drop([\"Default\"],1)\n",
    "valid_y = valid[\"Default\"]\n",
    "valid_x = valid.drop([\"Default\"],1)\n",
    "#imputation#\n",
    "train_x_imp, valid_x_imp = imputing(dataset_train_x,train_x), imputing(dataset_train_x,valid_x)\n",
    "dataset_train_x_imp = imputing(dataset_train_x,dataset_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_test = feature_engineering(performance_test,paiements_test,transactions_test, True)\n",
    "dataset_test_imputed = imputing(dataset_train_x,dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Gradient boosting model selection and submission\n",
    "bestGBclassifier = gradient_boosting_classifier(dataset_train_x_imp,dataset_train_y)\n",
    "ID = pd.Series(dataset_test.index)\n",
    "bestGBclassifier.fit(train_x_imp, train_y)\n",
    "GB_prediction = prediction(bestGBclassifier,valid_x_imp)\n",
    "submission_GB = submission_creator(ID,GB_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GB_prediction = prediction(bestGBclassifier,dataset_test_imputed)\n",
    "submission_GB = submission_creator(ID,GB_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## write_csv\n",
    "csv_write(submission_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
